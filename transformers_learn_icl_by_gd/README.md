# Transformers learn in-context by gradient descent
Notebooks for easy replication of the results in the paper [**Transformers learn in-context by gradient descent**](https://arxiv.org/abs/2212.07677).

As the naming suggests, the three notebooks can be used to reproduce the results for the

1. specific token construction where we concatenate in- and outputs.
2. usual token construction where we provide in- and outputs in neighbouring tokens.
3. experiments on non-linear regression tasks.

You can also use the following links to run the notebooks in Google Colab.

1. [normal_token_construct.ipynb](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/transformers_learn_icl_by_gd/normal_token_construct.ipynb)
2. [constructed_token_setup.ipynb](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/transformers_learn_icl_by_gd/constructed_token_setup.ipynb)
3. [non_linear_regression.ipynb](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/transformers_learn_icl_by_gd/non_linear_regression.ipynb)